---
title: 'MECH481A6: Engineering Data Analysis in R'
subtitle: 'Chapter 11 Homework: Modeling' 
author: 'Thomas Robert'
date: '`Dec. 11`'
output: pdf_document
---

```{r global-options, include=FALSE}
# set global options for figures, code, warnings, and messages
knitr::opts_chunk$set(fig.width=6, fig.height=4, fig.path="../figs/",
                      echo=FALSE, warning=FALSE, message=FALSE)
```

# Load packages

```{r load-packages, message=FALSE}
# load packages for current session
library(tidyverse) 
library(gridExtra)
library(car)
```

# Chapter 11 Homework

This homework will give you experience with OLS linear models and testing their assumptions.  

For this first problem set, we will examine issues of ***collinearity among predictor variables*** when fitting an OLS model with two variables. As you recall, assumption 3 from OLS regression requires there be no *collinearity* among predictor variables (the $X_i$'s) in a linear model.  The reason is that the model struggles to assign the correct $\beta_i$ values to each predictor when they are strongly correlated.   

## Question 1
Fit a series of three linear models on the `bodysize.csv` data frame using `lm()` with `height` as the dependent variable:  
  1. Model 1: use `waist` as the independent predictor variable:  
        - `formula = height ~ waist`   
  2. Model 2: use `mass` as the independent predictor variable:  
        - `formula = height ~ mass`  
  3. Model 3: use `mass + waist` as a linear combination of predictor variables:  
        - `formula = waist + mass`  
    
Report the coefficients for each of these models.  What happens to the sign and magnitude of the `mass` and `waist` coefficients when the two variables are included together?  Contrast that with the coefficients when they are used alone.

Evaluate assumption 3 about whether there is collinearity among these variables.  Do you trust the coefficients from model 3 after having seen the individual coefficients reported in models 1 and 2?


```{r ch11-homework-q1, echo=FALSE, include=FALSE}
bodysize_data <- read_csv("bodysize.csv")

model1 <- lm(height ~ waist, data = bodysize_data)
model2 <- lm(height ~ mass, data = bodysize_data)
model3 <- lm(height ~ waist + mass, data = bodysize_data)

summary(model1)$coefficients
summary(model2)$coefficients
summary(model3)

vif(model3)

# VIF value is over the threshold of 5, which is high enough to indicate muticollinearity. This value indicates that the variance is over 5x larger than it would be if the two predictors were completely uncorrelated. 

# No, I do not trust the individual coefficients from models 1 & 2 based on the VIF from model 3. This value indicates that height is predicted by waist + mass and that there is a strong enough correlation between the two that height depends on both, and neither one alone. 
```

## Question 2
Create a new variable in the `bodysize` data frame using `dplyr::mutate`. Call this variable `volume` and make it equal to $waist^2*height$.  Use this new variable to predict `mass`.  

```{r ch11-homework-q2}

bodysize_data <- bodysize_data %>%
  mutate(volume = (waist^2) * height)

modelq2 <- lm(mass ~ volume, data = bodysize_data)

summary(modelq2)

bodysize_data %>%
ggplot(aes(x = volume, y = mass)) + 
  geom_point(alpha = 0.5) + 
  stat_smooth(method = "lm", col = "red") +
  labs(
    title = "Prediction of Mass using Waist^2 * Height (Volume)",
    x = "Volume",
    y = "Mass"
  ) 

```

Does this variable explain more of the variance in `mass` from the NHANES data? How do you know? (hint: there is both *process* and *quantitative* proof here)

```{r ch11-homework-q2a}

# Yes it does. I know because the R^2 value is 0.87 compared to 0.43 which is much higher. 

```

Create a scatterplot of `mass` vs. `volume` to examine the fit.  Draw a fit line using `geom_smooth()`.

```{r ch11-homework-q2b}

# created my scatterplot in the earlier part of this question

```

## Question 3
Load the `cal_aod.csv` data file and fit a linear model with `aeronet` as the independent variable and `AMOD` as the independent variable. 
```{r ch11-homework-q3}
# load data


```

Evaluate model assumptions 4-7 from the coursebook.  Are all these assumptions valid? 

```{r ch11-homework-q3a}
#assumption 4: mean of residuals is zero


```

```{r ch11-homework-q3b}
#assumption 5: residuals are normally distributed


```

```{r ch11-homework-q3c}
#assumption 6: the error term is homoscedastic


```

```{r ch11-homework-q3d}
#assumption 7: no autocorrelation among residuals


```